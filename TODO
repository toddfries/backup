- load dumplist into an array vs for.. in $(sed ../dumplist)
- afs
  - permit specifying '*' for volume names, i.e. fdc.finance.*
- sanity checking of syntax, i.e. invalid typical things:
  sshdump://host/ (no user present)
  dump://user@host/ (user present)
- separate uncomp data vs uncomp speed units. example:
   02/06/2008 13:51:08 CST uncomp. 271.98GB vs comp. 260.63GB
   02/06/2008 13:51:08 CST uncomp. 0.00GB/s vs comp. 0.00GB/s
   02/06/2008 13:51:08 CST comp. 95.82%, ratio 4.18%
- automate samba password reset procedure
  echo "$oldpass\n$newpass\n$newpass" | smbpasswd -r servername -Uusername -s
- handle 'session setup failed: NT_STATUS_PASSWORD_EXPIRED'


- metadata
  - keep track of files that exist each backup
    - consider merits of storing as a list vs differential list?
    - notice when files move location
  - keep track of permissions and/or acl's where applicable
- backup the local system
  x dump://hostname/dir
    x hostname is only for the dir creation
    - fix the level>0 time issue
- remote backup security
  - timestamp/auth info is xfered by execing a shell remotely
  - create a wrapper program to check carefully
- concurrency
 - 1st implement rough `maxconcurrence' per backup set
  - look at fetchitdave for inspiration
 - 2nd implement `maxconcurrence' per host and per backup set
 - setup level 0 backups in one queue, non level 0 in another, to
   permit new hosts etc to not keep level>0 backups from happening for days
- move filelist creation earlier, so .host_filesyst.level.filelist is moved
   with the rest instead of created after the move
 - perhaps even create the filelist via 'tee' and a fifo or somesuch
   so that reading the data happens as is it written to the disk, not
   reading it back causing more disk io

- verify collections don't have more than one backup process running,
   so backups that take more than 24hrs will block other backups from
   running
  - almost makes some sense to have levels somewhat automatic, so one
    does not end up skipping level 1's for example because the night before's
    backup took too long
- backup-addhost / backup
 - implement 'df' output on the host to list partitions to potentially backup
 - permit copying the authorized_keys file into place and setting up the
   remote backup script

- implement archiving to tape or other storage, e.g. epitome
- update sshsmb to match smb regarding smbshare/smbdir
- write recover script to restore data
  - for 'vos dump' output, there is one line of text to remove, something like
     this:
       lzma -d < $file | dd bs=$(head -1 $file | wc -c) skip=1 | ...
    

- ability to backup databases (PostgreSQL, MySQL, etc)

As spoken to a client:
 - the ability to set a parallel variable per host and per backup system,
   to backup multiple systems simultaneously; serially backing up systems
   when one system hangs (i.e. dump -L on FreeBSD) does not let things
   backup as expected
 - the ability to set a timeout on each backup process if it has not returned
   data within a timeout interval  
 - creating a 'recover' command to automate much of the data recovering phase
 - make the .filelist files a db of some sort instead of a text list to
   quickly process large volumes during recovery
 - create the .filelist `on the fly' while doing the backup; currently it
   requires a separate pass
 - compress via 'gzip' on the client instead of via ssh -C on the client,
   decompressing on the backup host, and then re-compressing on the backup     
   host
 - permit alternate compression mechanisms, lzma is best but chews cpu like
   a banshee  
 - integrate gpg for encrypted public private key backups
 - integrate openssl for passphrase encrypted backups
 - implement archiving to tape

- check space used by the archive vs the data storage, if level0 is much
  larger and days since level0 is greater than x, new level0
- each system backed up can fit in one column, the stats output
  eventually shrink the output to columns
- use size of filesystem (`df' like info, have to implement for all
  filesystems supported, samba, afs, ..) to determine when to bump
  back to level 0
- when bumping levels, don't treat 0 special, any level that is
   a) bigger than its next lower level
   b) filesystem is smaller than the next lower level
   .. might need a bump back down
- permit specifying how many days to keep data around for,
  i.e. minimumdays=14, but be clear the higher the number the
  bigger the dumps will get
- per file extraction of all archives (i.e. for epitome)
- optional compression on the client or the server (especially
  useful for parallel operations and/or lzma -9)
- store stats about each backup, what type of compression, etc
- permit specifying a gpg key in the global conf per backup set
  to use locally or remotely, remote gpg encryption necessitates
  remote compression prior to encryption as well as remote file
  listing in a separate file descriptor

10/10/2008
x error cleanup, don't leave unfinished backups lying around
  x use trap

Prior to 10/10/2008
x show the time the backup is relative to, if not level 0
x permit using '-h 0' flag of dump
  (implemented by default, have to code more to make it optional)
x backup samba shares accessable only via smbclient on a remote system
  x sshsmb://user@hostname/authname@remotesambahost/remotesharename
